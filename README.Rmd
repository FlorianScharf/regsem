---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
```

# regsem: Regularized Structural Equation Modeling

This package is based on the paper by Jacobucci, Grimm, & McArdle (2016), available at: http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793. The package allows for using ridge or lasso penalties on specific parameters in general structural equation models. It is currently an extremely developmental version in both its implementation and theory behind the method.

## First: Installing the package
```{r,eval=FALSE}
devtools::install_github("RJacobucci/regsem")
library(regsem)
```
It is best to install directly from Github as submissions to CRAN take place only once every couple of months.

As a simple example, variable selection can be done in a simple one factor model.

### Example 1: Variable selection in a One Factor CFA

```{r}
library(regsem,quietly=T) # Depends on lavaan
HS <- data.frame(scale(HolzingerSwineford1939[,7:15]))
mod <- '
f =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9
'
# Recommended to specify meanstructure in lavaan
outt = cfa(mod,HS,meanstructure=TRUE)

fit1 <- regsem(outt,lambda=0.1,type="lasso")
summary(fit1)
```

By default, regsem penalizes only the factor loadings (all) unless otherwise specified. If specific factor loadings, or other parameters are desired for regularization, use pars_pen().

### Example 2: Penalizing specific parameters

```{r,eval=FALSE}
# find parameter numbers
parTable(outt)
# or
extractMatrices(outt) # from regsem, can look at par #'s directly in A or S matrix

# only penalize first 3 loadings
fit2 <- regsem(outt,lambda=0.1,type="lasso",pars_pen=c(1:3))
summary(fit2)
```

## Multiple Starting Values

As currently implemented, regsem() encounters difficulty in convergence, particularly as lambda increases. Therefore, it is almost always recommended to use multi_optim() with at least 10 different starting values.

### Example 3: Multiple starting values

```{r,eval=FALSE}
?multi_optim
fit.mult <- multi_optim(outt,lambda=0.15,max.try=100,type="lasso",
                        max.iter=100000,tol=1e-4)
summary(fit.mult) 

# get fit of model -- using bootstrapping (naive)
fit_indices(fit.mult,CV="boot")
# to use CV=T, need to pre=partition the sample and provide a test covariance matrix
```

This isn't a perfect solution to the convergence difficulties and will be further addressed with different types of optimization in the future. With ridge penalties, it currently works best to not use a gradient for optimization (gradFun="none"). However, for lasso penalties, it is currently recommended to use the gradient specification from von Oertzen & Brick (2014) sparse RAM matrix derivatives paper (gradFun="ram"). 

### Example 4: Testing multiple lambda values

```{r,eval=FALSE}
cv.out = cv_regsem(outt,type="ridge",gradFun="none",fit.ret2="boot",
                   n.lambda=40,mult.start=TRUE)

# see rmsea and BIC for each value of lambda
cv.out[[2]]

# lowest

min.bic <- min(cv.out[[2]][,"BIC"])
loc <- which(cv.out[[2]][,"BIC"] ==  min.bic)
cv.out[[2]][1,]
```



### Things that regsem does not currently support

+ missing data -- don't use missing="fiml" in lavaan code
+ outputting standard errors or p-values
+ multiple group models
+ categorical indicators -- will be treated as continuous

### Important things to note

+ generally, ridge penalties converge faster than lasso
+ fit indices should be interpreted with a grain of salt
+ using lasso, the degrees of freedom reflect the number of parameters not equal to zero
+ using ridge, the degrees of freedom don't change (will be modified in future)


### Questions, Suggestions, Errors?
send an email to: rcjacobuc@gmail.com

